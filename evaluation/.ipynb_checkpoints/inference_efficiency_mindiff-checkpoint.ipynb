{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34b09a-acf4-4f73-8c92-bfe8d113f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"/append/MINDiff/path\")\n",
    "from pipeline_stable_diffusion_mask import StableDiffusionMaskPipeline\n",
    "from set_attn_proc import set_mask_attn\n",
    "from mask_attention_processor import MaskAttnProcessor2_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfb0f1-b961-4fe0-996e-b8d5077bad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionMaskPipeline.from_pretrained(\n",
    "    \"/Input/model/path\",\n",
    "    torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = pipeline.unet\n",
    "unet = set_mask_attn(\n",
    "    unet=unet,\n",
    "    mask_resolution=16,\n",
    "    )\n",
    "mask_token_id = pipeline.tokenizer.convert_tokens_to_ids(\"sks</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b14a6-6872-4921-bbc6-aa1b45ff6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sd_latency(pipe, prompt, pre_prompt, num_images=1, repeat=10):\n",
    "    # Warm-up\n",
    "    MaskAttnProcessor2_0.mask = None\n",
    "    _ = pipe(prompt,\n",
    "             num_images_per_prompt=num_images,\n",
    "             mask_token_id=mask_token_id,\n",
    "             pre_prompt=pre_prompt,\n",
    "             attn_scale=1.0)\n",
    "    if hasattr(MaskAttnProcessor2_0, 'mask_buffer'):\n",
    "        del MaskAttnProcessor2_0.mask_buffer\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize() # Wait until GPU operations are complete\n",
    "\n",
    "    # Wait until GPU operations are complete\n",
    "    torch.cuda.reset_peak_memory_stats() # Reset peak memory usage from previous runs\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(repeat):\n",
    "        MaskAttnProcessor2_0.mask = None\n",
    "        _ = pipe(prompt,\n",
    "                 num_images_per_prompt=num_images,\n",
    "                 mask_token_id=mask_token_id,\n",
    "                 attn_scale=1.0,\n",
    "                 pre_prompt=pre_prompt)\n",
    "        if hasattr(MaskAttnProcessor2_0, 'mask_buffer'):\n",
    "            del MaskAttnProcessor2_0.mask_buffer\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    \n",
    "    avg_latency = (end - start) / repeat\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    \n",
    "    return avg_latency, peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0b4c0-98f5-4f22-a607-10c8d915bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency, memory = measure_sd_latency(pipeline, \"a sks backpack on a wooden table\", \"a sks backpack\", num_images=1, repeat=10)\n",
    "print(f\"Avg latency: {latency*1000:.2f} ms\")\n",
    "print(f\"Peak GPU memory: {memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42fd79-4434-4ea0-a3fe-e2752823dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sd_latency_event(pipe, prompt, pre_prompt, num_images=1, repeat=10):\n",
    "    MaskAttnProcessor2_0.mask = None\n",
    "    _ = pipe(prompt,\n",
    "             num_images_per_prompt=num_images,\n",
    "             mask_token_id=mask_token_id,\n",
    "             pre_prompt=pre_prompt,\n",
    "             attn_scale=1.0) # wram-up\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    total_time = 0.0\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start_event.record()\n",
    "        MaskAttnProcessor2_0.mask = None\n",
    "        _ = pipe(prompt,\n",
    "                 num_images_per_prompt=num_images,\n",
    "                 mask_token_id=mask_token_id,\n",
    "                 attn_scale=1.0,\n",
    "                 pre_prompt=pre_prompt)\n",
    "        end_event.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = start_event.elapsed_time(end_event)\n",
    "        total_time += elapsed\n",
    "\n",
    "    avg_latency = total_time / repeat\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    return avg_latency, peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaefba-37a6-437d-b277-002df5089e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency, memory = measure_sd_latency_event(pipeline, \"a sks backpack on a wooden table\", \"a sks backpack\", num_images=1, repeat=30)\n",
    "print(f\"Avg latency: {latency:.2f} ms\") # No need to multiply by 1000 since the result is already in milliseconds\n",
    "print(f\"Peak GPU memory: {memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a857f-6b9c-4a7e-9dfd-51f39588c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 2, 4, 8]\n",
    "prompt = \"a sks backpack on a wooden table\"\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "\n",
    "    print(f\"\\nBatch size: {batch_size}\")\n",
    "\n",
    "    # Warm-up run\n",
    "    MaskAttnProcessor2_0.mask = None\n",
    "    MaskAttnProcessor2_0.stack = None\n",
    "    _ = pipeline(prompt,\n",
    "                num_images_per_prompt=batch_size,\n",
    "                mask_token_id=mask_token_id,\n",
    "                attn_scale=1.0,\n",
    "                pre_prompt=\"a sks backpack\")\n",
    "    if hasattr(MaskAttnProcessor2_0, 'mask_buffer'):\n",
    "            del MaskAttnProcessor2_0.mask_buffer\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Start measurement\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    MaskAttnProcessor2_0.mask = None\n",
    "    _ = pipeline(prompt,\n",
    "                num_images_per_prompt=batch_size,\n",
    "                mask_token_id=mask_token_id,\n",
    "                attn_scale=1.0,\n",
    "                pre_prompt=\"a sks backpack\")\n",
    "    if hasattr(MaskAttnProcessor2_0, 'mask_buffer'):\n",
    "            del MaskAttnProcessor2_0.mask_buffer\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    elapsed = (end - start)\n",
    "    elapsed_ms = elapsed * 1000\n",
    "    fps = batch_size / elapsed\n",
    "    per_image_time = elapsed_ms / batch_size\n",
    "\n",
    "    print(f\"Time: {elapsed_ms:.2f}ms | FPS: {fps:.2f} | Per image: {per_image_time:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee81f98-5643-4f78-9e95-8e2109ff4a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreambooth",
   "language": "python",
   "name": "dreambooth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
