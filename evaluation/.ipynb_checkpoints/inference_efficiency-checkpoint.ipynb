{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34b09a-acf4-4f73-8c92-bfe8d113f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfb0f1-b961-4fe0-996e-b8d5077bad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"/input/model/path\",\n",
    "    torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b14a6-6872-4921-bbc6-aa1b45ff6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sd_latency(pipe, prompt, num_images=1, repeat=10):\n",
    "    # Warm-up\n",
    "    _ = pipe(prompt, num_images_per_prompt=num_images)\n",
    "    torch.cuda.synchronize() # Wait until GPU operations are complete\n",
    "\n",
    "    # Clear memory tracking\n",
    "    torch.cuda.reset_peak_memory_stats() # Reset peak memory usage from previous runs\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(repeat):\n",
    "        _ = pipe(prompt, num_images_per_prompt=num_images)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    \n",
    "    avg_latency = (end - start) / repeat\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    \n",
    "    return avg_latency, peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0b4c0-98f5-4f22-a607-10c8d915bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency, memory = measure_sd_latency(pipeline, \"a sks backpack on a wooden table\" , num_images=1, repeat=30)\n",
    "print(f\"Avg latency: {latency*1000:.2f} ms\")\n",
    "print(f\"Peak GPU memory: {memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42fd79-4434-4ea0-a3fe-e2752823dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_sd_latency_event(pipe, prompt, num_images=1, repeat=10):\n",
    "    pipe(prompt, num_images_per_prompt=num_images) # wram-up\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    total_time = 0.0\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start_event.record()\n",
    "        _ = pipe(prompt, num_images_per_prompt=num_images)\n",
    "        end_event.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = start_event.elapsed_time(end_event)\n",
    "        total_time += elapsed\n",
    "\n",
    "    avg_latency = total_time / repeat\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    return avg_latency, peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feaefba-37a6-437d-b277-002df5089e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency, memory = measure_sd_latency_event(pipeline, \"a sks backpack on a wooden table\" , num_images=1, repeat=30)\n",
    "print(f\"Avg latency: {latency:.2f} ms\") # No need to multiply by 1000 since the result is already in milliseconds\n",
    "print(f\"Peak GPU memory: {memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a857f-6b9c-4a7e-9dfd-51f39588c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 2, 4, 8]\n",
    "prompt = \"a sks backpack on a wooden table\"\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "\n",
    "    print(f\"\\nBatch size: {batch_size}\")\n",
    "\n",
    "    # Warm-up run\n",
    "    _ = pipeline(prompt, num_images_per_prompt=batch_size)\n",
    "\n",
    "    # Start measurement\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = pipeline(prompt, num_images_per_prompt=batch_size)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    elapsed = (end - start)\n",
    "    elapsed_ms = elapsed * 1000\n",
    "    fps = batch_size / elapsed\n",
    "    per_image_time = elapsed_ms / batch_size\n",
    "\n",
    "    print(f\"Time: {elapsed_ms:.2f}ms | FPS: {fps:.2f} | Per image: {per_image_time:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a309f6-fa4f-4498-b400-74b5558c447d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreambooth",
   "language": "python",
   "name": "dreambooth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
